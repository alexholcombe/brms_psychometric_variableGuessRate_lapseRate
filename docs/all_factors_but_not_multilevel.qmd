---
title: "Psychophysical model recovery using Bayesian brms"
format: html
---

To get started, we load the required packages.

```{r}
#| warning: false 
#| output: false
library(tidyverse)
library(brms)

source("../R/simulate_data.R") #Load my needed custom function
source("../R/psychometric_function.R") #Load my needed custom function
source("../R/helpers/checkCounterbalancing.R") #Load my needed custom function

set.seed(999) #ensures reproducibility for testing
```

# Create simulated trials

Set up simulated experiment design.

```{r}
#| echo: true 
laboratories<- c("Roudaia", "Holcombe")
subjPerGroup<- 50
trialsPerCondition<- 8
targetNumConds<- c(2,3)
#Array of speeds (not very realistic because mostly controlled by a staircase in actual experiment)
speeds<-seq(.02,1.8, length.out = 12) # trials at 12 different speeds between .02 and 1.8

```

In order to build and test our model in brms, we must first create a simulated data set that is similar to our actual experiment data. This allows us to confirm the brms model is working and successfully recovers the parameters we set before applying it to our real experimental data that has unknown parameter values. In the actual data, there will be many group-wise differences in location and scale parameters. The following simulated data only has explicit differences between the $\eta$ (location) of the two age groups (older vs younger).


```{r}
#| echo: false 

trials <- generate_conditions_both_labs(laboratories,subjPerGroup,trialsPerCondition,
                                        targetNumConds,speeds)

#Print number of unique values of each column
print('Number of values for each factor:')
numValsPerFactor<- trials |> summarise(across(everything(), ~ n_distinct(.))) |>
                              pivot_longer(everything())
print( numValsPerFactor )
checkCombosOccurEqually(trials |> filter(lab=="Holcombe"),
                        c("subjWithinGroup","num_targets","speed"),dropZeros=FALSE) 
checkCombosOccurEqually(trials |> filter(lab=="Holcombe"),
                        c("subjWithinGroup","num_targets","speed","gender","age_group"),dropZeros=FALSE) 
  
```


Determine how the parameters are different for each condition, lab, gender, etc.

Create between-subject variability within each group, to create a potential multilevel modeling advantage. Can do this by passing the participant number to the location-parameter calculating function. Because that function will be called over and over, separately, the location parameter needs to be a deterministic function of the participant number. So it needs to calculate a hash or something to determine the location parameter. E.g. it could calculate the remainder, but then the location parameters wouldn't be centered on the intended value for that condition. To do that, I think I need to know the number of participants in the condition, n, and number them 1..n. Then I can give e.g. participant 1 the extreme value on one side of the condition-determined value and give participant n the extreme value on the other side.
So I number participants separately even within age*gender to maintain the age and gender penalties, but not within speed, of course. Also not within num_targets or obj_per_ring.

Because the present purpose of numbering participants is to inject the right amount of between-participant variability, I will number the participants with a range of numbers that has  unit standard deviation. Will call this column "subjStandardized".

```{r}
#| echo: false

  #Create subject number within condition, which I'll use to create between-subject variance
  # that is not related to condition
  trials <- trials |> group_by(lab,age_group,gender) |> 
                      mutate( subjWithinCond = cur_group_id() )
  numSsPerCond <- length( sort(unique(trials$subjWithinCond)) )
  cat('Num participants per condition = ',numSsPerCond)
  
  #Renumber subjWithinCond to be standardized, centered on zero and have unit standard deviation, so that that person's unique params 
  # can be assigned by multiplying subjWithinCond by the variance, because then the standard
  # deviation 
  # adding it to the designated mean val.
  center_on_zero_and_standardize_std <- function(x,vals) {
    centered <- (x - mean(vals)) / sd(vals)
    return(centered)
  }

  trials<-trials |> 
    mutate( subj_std = center_on_zero_and_standardize_std(subjWithinCond,
                                                          unique(trials$subjWithinCond))
          )
  
  #I also need a trial number, so far I only have a trialThisCond column
  #Calculate a trial number numbering the entirety of the trials the subject is given
  #Assume the within-participant factors are obj_per_ring,num_targets, and speed
  trials <- trials |> group_by(lab,trialThisCond,age_group,gender,subjWithinCond) |> 
                      mutate(trial = row_number())
```


```{r}
#| echo: true 
lapse <- 0.05
sigma <- 0.2
sigma_between_participant_variance <- 0.2 

location_param_young_123targets <- c(1.7,1.0,0.8)
age_penalty <- 0.4 #Old people have worse limit by this much
gender_penalty <- 0.2
Holcombe_lab_penalty <- 0.2

#Using above parameters, need function to calculate a participant's location parameter
location_param_calculate<- function(num_targets,age_group,gender,lab,subjStandardized) {
  
  base_location_param <- location_param_young_123targets[num_targets]
  #calculate offset for this participant based on desired sigma_between_participant_variance
  location_param <- base_location_param + subjStandardized * sigma_between_participant_variance
  
  after_penalties <-location_param - if_else(age_group=="older",1,0) * age_penalty -
                      if_else(lab=="Holcombe",1,0) * Holcombe_lab_penalty -
                      if_else(gender=="F",1,0) * gender_penalty
  return (after_penalties)
}
```

Using the psychometric function, simulate whether participant is correct on each trial or not, and 
add that to the simulated data, trials

Before screwup of subjects in each condition, 50 of 'em
```{r}
data_one_condition <- trials |> 
    filter(num_targets==2,age_group=="younger",gender=="M",lab=="Holcombe")
table(data_one_condition$subjWithinGroup)
nrow(data_one_condition)
```

```{r}
#| echo: false

data_simulated<- trials |> mutate( chance_rate = 1/obj_per_ring,
      location_param = location_param_calculate(num_targets,age_group,gender,lab,subj_std) )

#Add column for the probability of each trial being correct based on our psychometric function
#then use that to add column to generate if the subject got the trial correct
data_simulated <- data_simulated %>%
  mutate(
    p_correct = psychometric_function(1/obj_per_ring,lapse,speed,location_param,sigma),
    correct = rbinom(n=length(p_correct), size=1, prob=p_correct)
  )
        
```    
    
After screwup
```{r}
#| echo: false
  
data_one_condition <- data_simulated |> 
  filter(num_targets==2,age_group=="younger",gender=="M",lab=="Holcombe")
table(data_one_condition$subjWithinCond)
nrow(data_one_condition)

#Looks like subjWithinCond not working, because only one of them and 50 subj

```

# Plot data

    upper_bound = 1 - L*(1-C)

```{r}
#| echo: false

gg<- ggplot(data_one_condition, #data_simulated,
            aes(x=speed,y=p_correct,linetype=age_group,color=factor(num_targets))) +
  #stat_summary(fun=mean,geom="point") +
  stat_summary(fun=mean,geom="line")  +
  facet_grid(lab~obj_per_ring) +
  labs(x = "Speed (revolutions per second)",
        y = "P(Correct)",
        title = "Simulated data") +
  theme_bw()

#Also show variability
range <- function(x) {
  data.frame(
    y = mean(x), ymin = min(x), ymax = max(x)
  )
}

SDrange <- function(x) {
  data.frame(
    y = mean(x),
    ymin = mean(x) - 0.5*sd(x),
    ymax = mean(x) + 0.5*sd(x)
  )
}

#Can show variability as range or as CI
gg<-gg+stat_summary(fun.data=range, geom="ribbon", color=NA, fill="grey80", alpha=0.2)
#Add 95% confidence ribbon to show variability. Don't understand why this doesn't work!
#gg<-gg+stat_summary(fun.data = mean_cl_normal, fun.args = list(conf.int = 0.99),
#                    geom="ribbon", fill = "grey80", alpha = 0.5)
show(gg)

#geom_hline(aes(yintercept = 1-lapse*(1-chance_rate), colour = "Upper Bound"), linetype = "dashed") +
    # geom_hline(aes(yintercept = 0.25 , colour = "Lower Bound"), linetype = "dashed") +
    # geom_vline(aes(xintercept = 0.9, colour = "eta")) +
    # geom_line(aes(x = speed, y = probability_correct,
    #               colour = "Probability Correct")) +
    # theme_light() +
    # lims(x = c(0,2.5), y = c(0,1)) +
    # scale_colour_manual(values = c("Upper Bound" = "blue", "Lower Bound" = "red", "eta" = "yellow", "Probability Correct" = "black")) +

```  

#Convert these variables from vectors to factors
data_simulated <- data_simulated %>%
  mutate(
    gender = as_factor(gender),
    age_group = as_factor(age_group),
    lab = as_factor(lab),
    gender = as_factor(gender)
  ) 




# Setting up our Model in brms

Setting a model formula in brms allows the use of multilevel models, where there is a hierarchical structure in the data. But at this point we haven't made the model multi-level as we have been concentrating on the basics of brms.

The bf() function of brms allows the specification of a formula. The parameter can be defined by population effects, where the parameter's effect is fixed, or group level effects where the parameter varies with a variable such as age. The "family" argument is a description of the response distribution and link function that the model uses. For more detailed information on setting up a formula and the different arguments in BRMS see<https://paulbuerkner.com/brms/reference/brmsformula.html>

The model we used is based off our psychometric function used to generate the data mentioned previously. The only explicitly-coded difference in our simulated data is in the location parameter of older vs younger. Thus, in addition to the psychometric function, we allowed $\eta$ and $\log(\sigma)$ to vary by age group in the model. Because the psychometric function doesn't map onto a canonical link function, we use the non-linear estimation capability of brms rather than linear regression with a link function.

*Alex's note: Using the nonlinear option is also what allowed us to set a prior on the thresholds  $\eta$, because we could then parametrize the function in terms of the x-intercept, whereas with the link-function approach, we are stuck with the conventional parameterization of a line, which [has a term for the y-intercept but not the x-intercept](https://bsky.app/profile/did:plc:kynaetyuzsp46xejc6mzpjle/post/3lg5lpartzs2z) *


```{r}
my_formula <- brms::bf( 
   correct ~ chance_rate + (1-chance_rate - lapseRate*(1-chance_rate)) * Phi(-(speed-eta)/exp(logSigma))
  ) 
my_formula <- my_formula$formula
  
my_brms_formula <- brms::bf(
  correct ~ chance_rate + (1-chance_rate - lapseRate * (1-chance_rate))*Phi(-(speed-eta)/exp(logSigma)), 
  eta ~ age_group + gender + lab,  # + gender + lab
  lapseRate ~ 1, #~1 means intercept only
  logSigma ~ age_group,
  family = bernoulli(link="identity"), #Otherwise the default link 'logit' would be applied
  nl = TRUE #non-linear model
)
```

# Set priors

See [visualize_and_select_priors.html](visualize_and_select_priors.html) for some motivation and visualisation.

```{r}
#| echo: true 

my_priors <- c(
  brms::set_prior("beta(2,33.33)", class = "b", nlpar = "lapseRate", lb = 0, ub = 1),
  brms::set_prior("uniform(0, 2.5)", class = "b", nlpar = "eta", lb = 0, ub = 2.5),
  brms::set_prior("uniform(-3, 1.6)", class = "b", nlpar = "logSigma", lb = -2, ub = 1.6) 
)

```

# Fitting Model to Simulated Data

Fitting the model gives an estimation of the average parameter value of the participants. The brm() function is used to fit the model based on the given formula, data and priors. Other arguments of brm can adjust the model fitting in various ways, for more information on each of the arguments see <https://paulbuerkner.com/brms/reference/brm.html>

```{r}
#| echo: false 

fit <- try( brm(
  my_brms_formula,
  data = data_simulated,
  prior = my_priors,
  silent = 0, #prints more detailed messages (helps debug)
  init = 0, #starting at
  chains = 4, # the more chains, the greater convergence in the model
  cores = 4, #how many parallel processes, speeds up model 
  iter = 2000, #default number of iterations 
  threads = threading(2) #how many cores work per chain
  ), silent=FALSE
)

if (inherits(fit, "try-error")) {
  message("An error occurred!")
} else {
  print('No errors when fitting.') #print(result)
}

summary(fit)
```
eta_intercept is the eta for the older group

eta_age_groupyounger represents the eta advantage for the younger age group. 
 
The logSigma estimate tends to have wide confidence intervals.

Check how close the fit estimates are to the true parameters.
```{r}
#| echo: false 

#Grab the estimates column of the parameter estimates
estimates<- as.data.frame( fixef(fit) )

#Create corresponding true values dataframe, to compare
trueVal_eta_young <- mean(location_param_young_123targets[2:3])
trueValsNamesInBrmParlance<-
  c("eta_Intercept","eta_age_groupyounger","eta_genderM", "eta_labRoudaia", "lapseRate_Intercept","logSigma_Intercept","logSigma_age_groupyonger")
trueVal<-c( 
     trueVal_eta_young - age_penalty, age_penalty, gender_penalty, Holcombe_lab_penalty, lapse, log(sigma), 0)
trueVal<- data.frame(trueVal)
rownames(trueVal) <- trueValsNamesInBrmParlance

estimates <- merge(estimates, trueVal, by = "row.names", sort=FALSE)
#Move trueVals column to second column so next to Estimate
estimates <- estimates %>% select(1, trueVal, everything())

estimates <- estimates %>%
  mutate(discrepancy = trueVal - Estimate, .after = Estimate)

#pretty print
estimates %>% dplyr::mutate(across(where(is.numeric), ~ round(.x, 2)))
```

The fit converged with no errors!! :grinning: (this doesn't always happen, when for example we use more complicated models)



The fitted model's estimated of the parameters are close to those used to generate the simulated data, meaning the model recovery was a success!!


