---
title: "Psychophysical model recovery using Bayesian brms"
format: html
---

TO-DO:

  - add change in psychometric function based on number of targets
  - Make it multilelvel, with multiple participants

To get started, we load the required packages.

```{r}
#| warning: false 
#| output: false
rm(list = ls())

library(tidyverse)
library(brms)

source("../R/simulate_data.R") #Load my needed custom function
source("../R/psychometric_function.R") #Load my needed custom function

set.seed(999) #ensures reproducibility for testing
```

# Create simulated trials

Set up simulated experiment design.

```{r}
#| echo: true 
laboratories<- c("Holcombe","Roudaia")
numSubjects<- 50
trialsPerCondition<- 8
targetNumConds<- c(2,3)
#Array of speeds (not very realistic because mostly controlled by a staircase in actual experiment)
speeds<-seq(.02,1.8, length.out = 12) # trials at 12 different speeds between .02 and 1.8

```

In order to build and test our model in brms, we must first create a simulated data set that is similar to our actual experiment data. This allows us to confirm the brms model is working and successfully recovers the parameters we set before applying it to our real experimental data that has unknown parameter values. In the actual data, there will be many group-wise differences in location and scale parameters. The following simulated data only has explicit differences between the $\eta$ (location) of the two age groups (older vs younger).


```{r}
#| echo: false 

trials <- generate_conditions(laboratories,numSubjects,trialsPerCondition,targetNumConds,speeds)

#Print number of unique values of each column
#print('Number of values for each factor:')
numValsPerFactor<- trials |> summarise(across(everything(), ~ n_distinct(.))) |>
                              pivot_longer(everything())
print( numValsPerFactor )

```

Choose values for psychometric function for younger and older

```{r}
#| echo: true 
lapse <- 0.05
sigma <- 0.2
location_param_young_123targets <- c(1.7,1.0,0.8)
age_penalty <- 0.4 #Old people have worse limit by this much
gender_penalty <- 0.1
Holcombe_lab_penalty <- 0.2

#Using above parameters, need function to calculate a participant's location parameter
location_param_calculate<- function(num_targets,age_group,gender,lab) {
  
  base_location_param <- location_param_young_123targets[num_targets]
  after_penalties <- base_location_param - if_else(age_group=="older",1,0) * age_penalty -
                    if_else(lab=="Holcombe",1,0) * Holcombe_lab_penalty -
                    if_else(gender=="F",1,0) * gender_penalty
  return (after_penalties)
}
```

Using the psychometric function, simulate whether participant is correct on each trial or not, and 
add that to the simulated data, trials

```{r}
#| echo: false

data_simulated<- trials

#Add column for the probability of each trial being correct based on our psychometric function
#then use that to add column to generate if the subject got the trial correct
data_simulated <- data_simulated %>%
  mutate(
    chance_rate = 1/obj_per_ring,
    location_param = location_param_calculate(num_targets,age_group,gender,lab),
    p_correct = psychometric_function(1/obj_per_ring,lapse,speed,location_param,sigma),
    correct = rbinom(n=length(p_correct), size=1, prob=p_correct)
  )
        
```    
    
# Plot data

upper_bound = 1 - L*(1-C)

```{r}
#| echo: false

#Calculate threshold (eta) for each group*condition

#data_one_subject_group <- data_simulated |> 
#  filter(age_group=="younger",gender=="M",lab=="Holcombe")
#data_one_condition <- data_one_subject_group |> filter(num_targets==2)

gg<- ggplot(data_simulated, #data_one_condition, 
            aes(x=speed,y=p_correct,linetype=age_group,color=factor(num_targets))) +
  #stat_summary(fun=mean,geom="point") +
  stat_summary(fun=mean,geom="line")  +
  facet_grid(lab~obj_per_ring) +
  labs(x = "Speed (revolutions per second)",
        y = "P(Correct)",
        title = "Simulated data") +
  theme_bw() + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) #remove gridlines

#Also show between-subject variability
range <- function(x) {
  data.frame(
    y = mean(x), ymin = min(x), ymax = max(x)
  )
}
SDrange <- function(x) {
  data.frame(
    y = mean(x),
    ymin = mean(x) - 0.5*sd(x),
    ymax = mean(x) + 0.5*sd(x)
  )
}

#Can show variability as range or as CI
#gg<-gg+stat_summary( aes(group=interaction(age_group,num_targets)),
#                     fun.data=range, geom="ribbon", color=NA, fill="grey80", alpha=0.2 )
#Add confidence interval ribbon to show variability.
gg<-gg+stat_summary( aes(group=interaction(age_group,num_targets)),
                    fun.data = mean_cl_normal, fun.args = list(conf.int = 0.95),
                    geom="ribbon", fill="grey80", color=NA, alpha = 0.5)
#Show floor  and ceiling with lines
gg<- gg + geom_hline( aes(yintercept = chance_rate),
                      colour = "purple", alpha=0.2 ) +
     geom_hline( aes(yintercept = 1-lapse*(1-chance_rate)), 
                 colour = "yellow3", alpha=0.8 )

#Add threshold (location_parameter) lines by calculating average for each group of subjects,
#including midpoint (threshold percent correct level)
calc_midpoint <- function( chance_rate, lapse ) {
  (chance_rate + 1-lapse*(1-chance_rate)) / 2
}

#calculate the threshold percent correct level, which differs by num_objects as well as lapse
loc_param_each_group <- data_simulated |>
        group_by(age_group, lab, num_targets) %>%
        summarise(location_param = mean(location_param, na.rm = TRUE),
                  #Calculate midpoint, but I'm actually not sure if f(location_param) = midpoint
                  midpoint =  mean( calc_midpoint(chance_rate,lapse) ), 
                  .groups = "drop")

gg<-  gg+ geom_segment(data = loc_param_each_group,
                   aes(x = location_param, xend = location_param,
                       y = 0, yend = midpoint,
                       color=factor(num_targets)),
                       linetype = "dashed",alpha=.4)#, color = "grey3")
#This removes the default padding/margin below the y-axis, so the plot area starts exactly at y = 0.
gg<- gg + scale_y_continuous(limits=c(0,1),expand = c(0, 0))
show(gg)


#geom_hline(aes(yintercept = 1-lapse*(1-chance_rate), colour = "Upper Bound"), linetype = "dashed") +
    # geom_hline(aes(yintercept = 0.25 , colour = "Lower Bound"), linetype = "dashed") +
    # geom_vline(aes(xintercept = 0.9, colour = "eta")) +
    # geom_line(aes(x = speed, y = probability_correct,
    #               colour = "Probability Correct")) +
    # theme_light() +
    # lims(x = c(0,2.5), y = c(0,1)) +
    # scale_colour_manual(values = c("Upper Bound" = "blue", "Lower Bound" = "red", "eta" = "yellow", "Probability Correct" = "black")) +

```  

# Setting up our Model in brms

Setting a model formula in brms allows the use of multilevel models, where there is a hierarchical structure in the data. But at this point we haven't made the model multi-level as we have been concentrating on the basics of brms.

The bf() function of brms allows the specification of a formula. The parameter can be defined by population effects, where the parameter's effect is fixed, or group level effects where the parameter varies with a variable such as age. The "family" argument is a description of the response distribution and link function that the model uses. For more detailed information on setting up a formula and the different arguments in BRMS see<https://paulbuerkner.com/brms/reference/brmsformula.html>

The model we used is based off our psychometric function used to generate the data mentioned previously. The only explicitly-coded difference in our simulated data is in the location parameter of older vs younger. Thus, in addition to the psychometric function, we allowed $\eta$ and $\log(\sigma)$ to vary by age group in the model. Because the psychometric function doesn't map onto a canonical link function, we use the non-linear estimation capability of brms rather than linear regression with a link function.

*Alex's note: Using the nonlinear option is also what allowed us to set a prior on the thresholds  $\eta$, because we could then parametrize the function in terms of the x-intercept, whereas with the link-function approach, we are stuck with the conventional parameterization of a line, which [has a term for the y-intercept but not the x-intercept](https://bsky.app/profile/did:plc:kynaetyuzsp46xejc6mzpjle/post/3lg5lpartzs2z) *


```{r}
my_formula <- brms::bf( 
   correct ~ chance_rate + (1-chance_rate - lapseRate*(1-chance_rate)) * Phi(-(speed-eta)/exp(logSigma))
  ) 
my_formula <- my_formula$formula
  
my_brms_formula <- brms::bf(
  correct ~ chance_rate + (1-chance_rate - lapseRate * (1-chance_rate))*Phi(-(speed-eta)/exp(logSigma)), 
  eta ~ age_group + gender + lab,  # + gender + lab
  lapseRate ~ 1, #~1 means intercept only
  logSigma ~ 1,#age_group,
  family = bernoulli(link="identity"), #Otherwise the default link 'logit' would be applied
  nl = TRUE #non-linear model
)
```

# Set priors

See [visualize_and_select_priors.html](visualize_and_select_priors.html) for some motivation and visualisation.

```{r}
#| echo: true 

my_priors <- c(
  brms::set_prior("beta(2,33.33)", class = "b", nlpar = "lapseRate", lb = 0, ub = 1),
  brms::set_prior("uniform(0, 2.5)", class = "b", nlpar = "eta", lb = 0, ub = 2.5),
  brms::set_prior("uniform(-3, 1.6)", class = "b", nlpar = "logSigma", lb = -2, ub = 1.6) 
)

```

# Set priors

See [visualize_and_select_priors.html](visualize_and_select_priors.html) for motivation.

```{r}
#| echo: false 
lapse_param1<- 2
lapse_param2<- 33.33

eta_param1<- 0
eta_param2<- 2.5

logsigma_param1<- -3
logsigma_param2<- 1.6

#brms can't evaluate parameters in the prior setting so one has to resort to sprintf-ing a string
lapseRate_prior_distribution<- sprintf("beta(%s, %s)", lapse_param1, lapse_param2)
eta_prior_distribution<-  sprintf("uniform(%s, %s)", eta_param1, eta_param2)
logsigma_prior_distribution<- sprintf("uniform(%s, %s)", logsigma_param1, logsigma_param2)

# my_priors <- c(
#   brms::set_prior(lapseRate_prior_distribution, class = "b", nlpar = "lapseRate", lb = 0, ub = 1),
#   brms::set_prior(eta_prior_distribution, class = "b", nlpar = "eta", lb = 0, ub = 2.5),
#   brms::set_prior(logsigma_prior_distribution, class = "b", nlpar = "logSigma", lb = -2, ub = 1.6)
# )

# define my priors
my_priors <- c( brms::prior_string(lapseRate_prior_distribution, nlpar="lapseRate", 
                            class = "b", lb=0,ub=1),
                brms::prior_string(eta_prior_distribution, nlpar="eta", 
                            class = "b", lb=0,ub=2.5),
                brms::prior_string(logsigma_prior_distribution, nlpar="logSigma", 
                            class="b",lb=-2,ub=1.6)  )

```

Visualize priors, by plotting them all together (with arbitrary height).

```{r}
#| echo: false

# Create tibbles for each prior with a parameter label
prior_lapse <- tibble(
  x = seq(0, 1, length.out = 500),
  y = dbeta(x, lapse_param1, lapse_param2),
  parameter = "lapse"
) |>
  mutate(y = y / max(y)) #Normalize so has peak of 1

prior_location <- tibble(
  x = seq(-1, 10, length.out = 500),
  y = dunif(x, eta_param1, eta_param2),
  parameter = "location"
) |>
  mutate(y = y / max(y) * 0.95) #Normalize so has peak of 0.95 (to avoid overlap)

prior_scale <- tibble(
  x = seq(-5, 5, length.out = 500),
  y = dunif(x, logsigma_param1, logsigma_param2),
  parameter = "log(sigma)"
) |>
  mutate(y = y / max(y) * 0.92) #Normalize so has peak of 0.92 (to avoid overlap)

# Combine all priors
priors_all <- bind_rows(prior_lapse, prior_location, prior_scale)

# Plot
ggplot(priors_all, aes(x = x, y = y, color = parameter)) +
  geom_line() +
  theme_light() +
  labs(
    x = "Parameter value",
    y = "Density",
    color = "Parameter"
  ) + 
  coord_cartesian( xlim=c(-4,4) ) +
  theme_bw() + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) #remove gridlines
```


# Fitting Model to Simulated Data

Fitting the model gives an estimation of the average parameter value of the participants. The brm() function is used to fit the model based on the given formula, data and priors. Other arguments of brm can adjust the model fitting in various ways, for more information on each of the arguments see <https://paulbuerkner.com/brms/reference/brm.html>

```{r}
#| echo: false 

fit <- try( brm(
  my_brms_formula,
  data = data_simulated,
  prior = my_priors,
  silent = 0, #prints more detailed messages (helps debug)
  init = 0, #starting at
  chains = 4, # the more chains, the greater convergence in the model
  cores = 4, #how many parallel processes, speeds up model 
  iter = 2000, #default number of iterations 
  threads = threading(2) #how many cores work per chain
  ), silent=FALSE
)

if (inherits(fit, "try-error")) {
  message("An error occurred!")
} else {
  print('No errors when fitting.') #print(result)
}

summary(fit)
```
eta_intercept is the eta for the older group

eta_age_groupyounger represents the eta advantage for the younger age group. 
 
The logSigma estimate tends to have wide confidence intervals.

Check how close the fit estimates are to the true parameters.
```{r}
#| echo: false 

#Grab the estimates column of the parameter estimates
estimates<- as.data.frame( fixef(fit) )

#Create corresponding true values dataframe, to compare
trueVal_eta_young <- mean(location_param_young_123targets[2:3])
trueValsNamesInBrmParlance<-
  c("eta_Intercept","eta_age_groupyounger","eta_genderM", "eta_labRoudaia", "lapseRate_Intercept","logSigma_Intercept","logSigma_age_groupyonger")
trueVal<-c( 
     trueVal_eta_young - age_penalty, age_penalty, gender_penalty, Holcombe_lab_penalty, lapse, log(sigma), 0)
trueVal<- data.frame(trueVal)
rownames(trueVal) <- trueValsNamesInBrmParlance

estimates <- merge(estimates, trueVal, by = "row.names", sort=FALSE)
#Move trueVals column to second column so next to Estimate
estimates <- estimates %>% select(1, trueVal, everything())

estimates <- estimates %>%
  mutate(discrepancy = trueVal - Estimate, .after = Estimate)

#pretty print
estimates %>% dplyr::mutate(across(where(is.numeric), ~ round(.x, 2)))
```

The fit converged with no errors!! :grinning: (this doesn't always happen, when for example we use more complicated models)



The fitted model's estimated of the parameters are close to those used to generate the simulated data, meaning the model recovery was a success!!


