---
title: "logSigma estimation seems biased high"
format: html
---

Typically brms estimates logSigma as -1.41 instead of true value of -1.67.

Will generate fake data that includes differences between participants, but doesn't model them.

To get started, we load the required packages.

```{r}
#| warning: false 
#| output: false
rm(list = ls())

library(tidyverse)
library(brms)
library(here) #For finding root directory

source( here("R","simulate_data_no_underscores.R") )  #Load my needed custom function
source( here("R","psychometric_function.R") ) #Load my needed custom function

set.seed(989) #ensures reproducibility for testing

break_brms_by_making_numTargets_numeric<- FALSE #Change factor to numeric, right before doing formula fit, so 
   #I know that it's brms that's the problem rather than the rest of my code
```

# Create simulated trials

Set up simulated experiment design.

Maybe make sure the reference group (that the Intercept estimate pertains to) is the expected-worse group. This is in case I later set priors on the coefficients, so that I can set the priors to all be positive to be less confusing.

Supposedly the levels order determines what brms sets the reference group to.

```{r}
#| echo: true 

numTargetsConds<- factor( c("three", "two"),
                         levels=c("three", "two") ) #This defines factor order. Worst is first.
numSubjects<- 25
trialsPerCondition<- 30#20
laboratories<- factor( c("Holcombe", "Roudaia"),
                         levels=c("Holcombe", "Roudaia") ) #This defines the factor order

#Array of speeds (not very realistic because mostly controlled by a staircase in actual experiment)
speeds<-seq(.02,1.8, length.out = 12) # trials at 12 different speeds between .02 and 1.8
```

In order to build and test our model in brms, we must first create a simulated data set that is similar to our actual experiment data. This allows us to confirm the brms model is working and successfully recovers the parameters we set before applying it to our real experimental data that has unknown parameter values. In the actual data, there will be many group-wise differences in location and scale parameters. The following simulated data only has explicit differences between the $\eta$ (location) of the two age groups (older vs younger).

Values per factor
```{r}
#| echo: false 

trials <- generate_conditions(laboratories,numTargetsConds,numSubjects,trialsPerCondition,speeds)

#Print number of unique values of each column
#print('Number of values for each factor:')
numValsPerFactor<- trials |> summarise(across(everything(), ~ n_distinct(.))) |>
                              pivot_longer(everything())
print( numValsPerFactor )

```
Create between-subject variability within each group, to create a potential multilevel modeling advantage. Can do this by passing the participant number to the location-parameter calculating function. Because that function will be called over and over, separately, the location parameter needs to be a deterministic function of the participant number. So it needs to calculate a hash or something to determine the location parameter. E.g. it could calculate the remainder, but then the location parameters wouldn't be centered on the intended value for that condition. To do that, I think I need to know the number of participants in the condition, n, and number them 1..n. Then I can give e.g. participant 1 the extreme value on one side of the condition-determined value and give participant n the extreme value on the other side.
So I number participants separately even within age*gender to maintain the age and gender penalties, but not within speed, of course. Also not within targetLoad or obj_per_ring.

Because the present purpose of numbering participants is to inject the right amount of between-participant variability, I will number the participants with a range of numbers that has  unit standard deviation. Will call this column "subjStandardized".

```{r}
#| echo: false

  #Renumber subjWithinCond to be standardized, centered on zero and have unit standard deviation, so that that person's unique params 
  # can be assigned by multiplying subjWithinCond by the variance, because then the standard
  # deviation 
  # adding it to the designated mean val.
  center_on_zero_and_standardize_std <- function(x,vals) {
    centered <- (x - mean(vals)) / sd(vals)
    return(centered)
  }

  trials<-trials |> 
    mutate( subj_standardzd = center_on_zero_and_standardize_std(subjWithinGroup,
                                                          unique(trials$subjWithinGroup))
          )
  
  #In case I need trial number, so far I only have a trialThisCond column
  #Calculate a trial number numbering the entirety of the trials the subject is given
  #Assume the within-participant factors are obj_per_ring,targetLoad, and speed
  trials <- trials |> group_by(lab,numTargets,ageGroup,gender,subjWithinGroup) |> 
                      mutate(trial = row_number())
```

Choose values for psychometric function for younger and older

```{r}
#| echo: true 
lapse <- 0.05
sigma <- 0.2

location_param_base<-1.2 #location_param_young_123targets <- c(1.7,1.0,0.8)
one_less_target_benefit<-0.4 #0.3 #penalty for additional target
youth_benefit <- 0 #0.4 #Old people have worse limit by this much
male_benefit <-0 #0.15 #Female worse by this much
Roudaia_lab_benefit <- 0 #0.2
#Set parameters for differences between Ss in a group
eta_between_subject_sd <- 0 #0.1 

#Using above parameters, need function to calculate a participant's location parameter
#Include optional between_subject_variance
location_param_calculate<- function(numTargets,ageGroup,gender,lab,
                                    subjStandardized,eta_between_subject_sd) {
  
  base_location_param <- location_param_base# location_param_young_123targets[targetLoad]
  #calculate offset for this participant based on desired sigma_between_participant_variance
  location_param <- base_location_param + 
                    subjStandardized * eta_between_subject_sd
  
  after_penalties <-location_param + 
                      if_else(lab=="Roudaia",1,0) * Roudaia_lab_benefit +
                      if_else(ageGroup=="younger",1,0) * youth_benefit +
                      if_else(numTargets=="two",1,0) * one_less_target_benefit +
                      if_else(gender=="M",1,0) * male_benefit
  return (after_penalties)
}

#Need version with fixed eta_between_subject_sd for use in mutate
location_param_calc_for_mutate<- function(numTargets,ageGroup,gender,lab,
                                               subjStandardized) {
  location_param_calculate(numTargets,ageGroup,gender,lab,
                           subjStandardized, eta_between_subject_sd)
}
  
```


Using the psychometric function, simulate whether participant is correct on each trial or not, and add that to the simulated data.

```{r}
#| echo: false

data_simulated<- trials

#Add column for the probability of each trial being correct based on our psychometric function
#then use that to add column to generate if the subject got the trial correct
data_simulated <- data_simulated %>%
  mutate(
    chance_rate = 1/obj_per_ring,
    location_param = location_param_calc_for_mutate(numTargets,ageGroup,gender,
                                                    lab,subj_standardzd),
    p_correct = psychometric_function(1/obj_per_ring,lapse,speed,location_param,sigma),
    correct = rbinom(n=length(p_correct), size=1, prob=p_correct)
  )

#Create qualitative target load in case it being numeric is the problem for brms
#data_simulated<- data_simulated |> mutate( target_load = 
#                                      if_else(targetLoad==2,"less","more") )

```    
    
    
# Plot data

upper_bound = 1 - L*(1-C)

```{r}
#| echo: false

#Calculate threshold (eta) for each group*condition

#data_one_subject_group <- data_simulated |> 
#  filter(ageGroup=="younger",gender=="M",lab=="Holcombe")
#data_one_condition <- data_one_subject_group |> filter(targetLoad==2)

gg<- ggplot(data_simulated, #data_one_condition, 
            aes(x=speed,y=p_correct,linetype=ageGroup,color=factor(numTargets))) +
  #stat_summary(fun=mean,geom="point") +
  stat_summary(fun=mean,geom="line")  +
  facet_grid(lab~obj_per_ring) +
  labs(x = "Speed (revolutions per second)",
        y = "P(Correct)",
        title = "Simulated data") +
  theme_bw() + 
  theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank()) #remove gridlines

#Also show between-subject variability
range <- function(x) {
  data.frame(
    y = mean(x), ymin = min(x), ymax = max(x)
  )
}
SDrange <- function(x) {
  data.frame(
    y = mean(x),
    ymin = mean(x) - 0.5*sd(x),
    ymax = mean(x) + 0.5*sd(x)
  )
}

#Can show variability as range or as CI
gg<-gg+stat_summary( aes(group=interaction(ageGroup,numTargets)),
                     fun.data=SDrange, geom="ribbon", color=NA, fill="grey80", alpha=0.2 )
#Add confidence interval ribbon to show variability.
#gg<-gg+stat_summary( aes(group=interaction(ageGroup,targetLoad)),
#                    fun.data = mean_cl_normal, fun.args = list(conf.int = 0.95),
#                    geom="ribbon", fill="grey80", color=NA, alpha = 0.5)
#Show floor  and ceiling with lines
gg<- gg + geom_hline( aes(yintercept = chance_rate),
                      colour = "purple", alpha=0.2 ) +
     geom_hline( aes(yintercept = 1-lapse*(1-chance_rate)), 
                 colour = "yellow3", alpha=0.8 )

#Add threshold (location_parameter) lines by calculating average for each group of subjects,
#including midpoint (threshold percent correct level)
calc_midpoint <- function( chance_rate, lapse ) {
  (chance_rate + 1-lapse*(1-chance_rate)) / 2
}

#calculate the threshold percent correct level, which differs by num_objects as well as lapse
loc_param_each_group <- data_simulated |>
        group_by(ageGroup, lab, numTargets) %>%
        summarise(location_param = mean(location_param, na.rm = TRUE),
                  #Calculate midpoint, but I'm actually not sure if f(location_param) = midpoint
                  midpoint =  mean( calc_midpoint(chance_rate,lapse) ), 
                  .groups = "drop")

gg<-  gg+ geom_segment(data = loc_param_each_group,
                   aes(x = location_param, xend = location_param,
                       y = 0, yend = midpoint,
                       color=factor(numTargets)),
                       linetype = "dashed",alpha=.4)#, color = "grey3")
#This removes the padding/margin below the y-axis, so the plot area starts exactly at y = 0.
gg<- gg + scale_y_continuous(limits=c(0,1),expand = c(0, 0))
show(gg)


#geom_hline(aes(yintercept = 1-lapse*(1-chance_rate), colour = "Upper Bound"), linetype = "dashed") +
    # geom_hline(aes(yintercept = 0.25 , colour = "Lower Bound"), linetype = "dashed") +
    # geom_vline(aes(xintercept = 0.9, colour = "eta")) +
    # geom_line(aes(x = speed, y = probability_correct,
    #               colour = "Probability Correct")) +
    # theme_light() +
    # lims(x = c(0,2.5), y = c(0,1)) +
    # scale_colour_manual(values = c("Upper Bound" = "blue", "Lower Bound" = "red", "eta" = "yellow", "Probability Correct" = "black")) +

```  

# Setting up our Model in brms

Setting a model formula in brms allows the use of multilevel models, where there is a hierarchical structure in the data. But at this point we haven't made the model multi-level as we have been concentrating on the basics of brms.

The bf() function of brms allows the specification of a formula. The parameter can be defined by population effects, where the parameter's effect is fixed, or group level effects where the parameter varies with a variable such as age. The "family" argument is a description of the response distribution and link function that the model uses. For more detailed information on setting up a formula and the different arguments in BRMS see<https://paulbuerkner.com/brms/reference/brmsformula.html>

The model we used is based off our psychometric function used to generate the data mentioned previously. The only explicitly-coded difference in our simulated data is in the location parameter of older vs younger. Thus, in addition to the psychometric function, we allowed $\eta$ and $\log(\sigma)$ to vary by age group in the model. Because the psychometric function doesn't map onto a canonical link function, we use the non-linear estimation capability of brms rather than linear regression with a link function.

*Alex's note: Using the nonlinear option is also what allowed us to set a prior on the thresholds  $\eta$, because we could then parametrize the function in terms of the x-intercept, whereas with the link-function approach, we are stuck with the conventional parameterization of a line, which [has a term for the y-intercept but not the x-intercept](https://bsky.app/profile/did:plc:kynaetyuzsp46xejc6mzpjle/post/3lg5lpartzs2z) *


```{r}
my_formula <- brms::bf( 
   correct ~ chance_rate + (1-chance_rate - lapseRate*(1-chance_rate)) * Phi(-(speed-eta)/exp(logSigma))
  ) 
my_formula <- my_formula$formula

#If don't want to fit lapseRate, need to supply it
#data_simulated$lapseRate<- lapse
#If I don't want to fit eta, need to supply it
true_eta<- location_param_base
data_simulated$eta<- true_eta

my_brms_formula <- brms::bf(
  correct ~ chance_rate + (1-chance_rate - lapseRate * (1-chance_rate))*Phi(-(speed-eta)/exp(logSigma)), 
  #eta ~ 1, #lab + ageGroup + numTargets + gender,
  lapseRate ~ 1, #~1 estimates intercept only
  logSigma ~ 1,#ageGroup,
  family = bernoulli(link="identity"), #Otherwise the default link 'logit' would be applied
  nl = TRUE #non-linear model
)
```

# Set priors

See [visualize_and_select_priors.html](visualize_and_select_priors.html) for motivation.

```{r}
#| echo: false 

# Specify distribution names and parameters
lapseRate_prior_dist_name <- "beta"
lapse_param1 <- 2
lapse_param2 <- 33.33

logsigma_uniform_param1 <- -3
logsigma_uniform_param2 <- 1.6

logsigma_normal_param1 <- log(.3) #true sigma=.2
logsigma_normal_param2 <- 1.5

logsigma_prior_dist_name <- "normal"


eta_uniform_param1 <- 0
eta_uniform_param2 <- 2.5

eta_gaussian_param1 <- 1.0
eta_gaussian_param2 <- 2
eta_prior_dist_name <- "normal"


eta_differences_prior_dist_name <- "normal"
eta_diffs_gaussian_param1 <- 0
eta_diffs_gaussian_param2 <- 2

#default_priors<-    brms::default_prior(my_brms_formula, data = data_simulated   )
#print( default_priors )

# Build prior distribution strings for brms
lapseRate_prior_distribution <- sprintf("%s(%s, %s)", lapseRate_prior_dist_name,
                                        lapse_param1, lapse_param2)
logsigma_prior_distribution <- sprintf("%s(%s, %s)", logsigma_prior_dist_name,
                                       logsigma_normal_param1, logsigma_normal_param2)
eta_prior_distribution <- sprintf("%s(%s, %s)", eta_prior_dist_name,
                                  eta_gaussian_param1, eta_gaussian_param2)
eta_differences_prior_distribution <- sprintf("%s(%s, %s)",
                                      eta_differences_prior_dist_name, eta_diffs_gaussian_param1, eta_diffs_gaussian_param2)

#Set up actual priors for various coefficients and intercepts
lapseRate_prior<- brms::set_prior(lapseRate_prior_distribution,
                          class = "b", nlpar = "lapseRate", lb = 0, ub = 1)
eta_prior<- brms::set_prior(eta_prior_distribution,
              coef="Intercept", #This means the prior will only apply to eta_Intercept
              nlpar = "eta")#, lb = 0, ub = 2.5),
logSigma_prior<- brms::set_prior(logsigma_prior_distribution, 
                    class = "b", nlpar = "logSigma", lb = -2, ub = 1.6)
ageGroup_prior<- brms::set_prior(eta_differences_prior_distribution, 
                          coef="ageGroupyounger",
                          nlpar = "eta")
lab_prior<- brms::set_prior(eta_differences_prior_distribution, 
                          coef="labRoudaia",
                          nlpar = "eta")
lab_prior<- brms::set_prior(eta_differences_prior_distribution, 
                          coef="numTargetstwo",
                          nlpar = "eta")
lab_prior<- brms::set_prior(eta_differences_prior_distribution, 
                          coef="genderM",
                          nlpar = "eta")

my_priors <- c( logSigma_prior, lapseRate_prior #, eta_prior  #Intercept priors
                #ageGroup_prior, lab_prior #coefficient priors
                )

```

Plot the priors

```{r}
#| echo: false 

prior_lapse <- tibble(
  x = seq(0, 1, length.out = 500),
  y = if (lapseRate_prior_dist_name == "beta") {
    dbeta(x, lapse_param1, lapse_param2)
  } else {
    NA_real_
  },
  parameter = "lapse"
) |> mutate(y = y / max(y))

prior_location <- tibble(
  x = seq(-1, 10, length.out = 500),
  y = if (eta_prior_dist_name == "normal") {
    dnorm(x, eta_gaussian_param1, eta_gaussian_param2)
  } else if (eta_prior_dist_name == "uniform") {
    dunif(x, eta_uniform_param1, eta_uniform_param2)
  } else {
    NA_real_
  },
  parameter = "location"
) |> mutate(y = y / max(y) * 0.95)

prior_location_diff <- tibble(
  x = seq(-4, 10, length.out = 500),
  y = if (eta_differences_prior_dist_name == "normal") {
    dnorm(x, eta_diffs_gaussian_param1, eta_diffs_gaussian_param2)
  } else {
    NA_real_
  },
  parameter = "location group difference"
) |> mutate(y = y / max(y) * 0.95)

prior_scale <- tibble(
  x = seq(-5, 5, length.out = 500),
  y = if (logsigma_prior_dist_name == "uniform") {
    dunif(x, logsigma_param1, logsigma_param2)
  } else if (logsigma_prior_dist_name == "normal") {
    dnorm(x, logsigma_normal_param1, logsigma_normal_param2)
  } else {
    NA_real_
  },
  parameter = "log(sigma)"
) |> mutate(y = y / max(y) * 0.92)

# Combine all priors
priors_all <- bind_rows(prior_lapse, prior_location, prior_location_diff, prior_scale)

# Plot
ggplot(priors_all, aes(x = x, y = y, color = parameter)) +
  geom_line() +
  theme_light() +
  labs(
    x = "Parameter value",
    y = "Density",
    color = "Parameter"
  ) + 
  coord_cartesian(xlim = c(-4, 4)) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

# Fitting Model to Simulated Data

Fitting the model gives an estimation of the average parameter value of the participants. The brm() function is used to fit the model based on the given formula, data and priors. Other arguments of brm can adjust the model fitting in various ways, for more information on each of the arguments see <https://paulbuerkner.com/brms/reference/brm.html>

```{r}
#| echo: true 

if (break_brms_by_making_numTargets_numeric) {
  #Make numeric version of numTargets
  data_simulated <- data_simulated |>
    mutate(targets = case_when(
      numTargets == "two" ~ 2,
      numTargets == "three" ~ 3,
      TRUE ~ 0
    ))
  #Delete old column
  data_simulated$numTargets <- NULL #delete column
  data_simulated <- data_simulated %>%
    rename(numTargets = targets)
}
```

```{r}
#| echo: false 
# 
print('Show stan code . But I dont know how to interrogate it to see the factors.')

stan_code <- make_stancode(
      formula = my_brms_formula,
      data = data_simulated,
      prior = my_priors,
      silent = 0, #prints more detailed messages (helps debug)
      init = 0, #starting at 0 recommended "if chains do not initialize or behave well"
      chains = 2,#3, #4 is the default to check consistent convergence
      iter = 300 #2000 is default number of iterations
)

cat(stan_code)
```

```{r}
#| echo: false 

cores_available <- parallel::detectCores()
#set mc.cores option which is used by brms by default 
options(mc.cores = cores_available)

time <- try( system.time({ 
  fit <-
    brm(
      formula = my_brms_formula,
      data = data_simulated,
      prior = my_priors,
      silent = 0, #prints more detailed messages (helps debug)
      #init = 0, #
      chains = 2,#3, #4 is the default to check consistent convergence
      iter = 700, #2000 is default number of iterations 
      save_warmup = TRUE
    ) 
  }), #end of system.time
 silent=FALSE) #End of try

cat('Time taken (min) =', round( time["elapsed"] / 60. ,1) )

if (inherits(time, "try-error")) {
  message("An error occurred!")
} else {
  print('No errors when fitting.')
}

print( summary(fit) )
```


The logSigma estimate tends to have wide confidence intervals, and be biased less negative?

Check how close the fit estimates are to the true parameters.

First report on logSigma.

```{r}
#| echo: false 

#Grab the estimates column of the parameter estimates
estimates<- as.data.frame( fixef(fit) )

#brms' estimate for the reference group
eta_logSigma<- estimates["logSigma_Intercept","Estimate"]

true_logSigma<- log(sigma)

discrepancy<- eta_logSigma - true_logSigma 
if (abs(discrepancy)<0.1) {
  print("Success. Looks like I/brms calculated/estimated the reference group logSigma correctly.")
} else {
  cat("Discrepancy of",round(discrepancy,2),
      "between true logSigma,",round(true_logSigma,2),
        "and what brms estimated,",round(eta_logSigma,2))
}
  
```

Now report on eta_Intercept (if estimated).

```{r}
#| echo: false 

if (any(str_detect(row.names(estimates),"eta_Intercept"))) {
  
  #Grab the estimates column of the parameter estimates
  estimates<- as.data.frame( fixef(fit) )
  
  #brms' estimate for the reference group
  eta_Intercept<- estimates["eta_Intercept","Estimate"]
  
  #Get the true eta used to generate the data
  data_eta<- data_simulated$location_param
  if ( length(unique(data_eta))  > 1 ) {
    print('Expecting the same eta for all participants, you must have mixed conditions together.')
  }
  true_eta_Intercept<- data_eta[1]
  
  discrepancy<- eta_Intercept - true_eta_Intercept 
  if (abs(discrepancy)<0.1) {
    print("Success. Looks like I/brms calculated/estimated the eta_Intercept correctly.")
  } else {
    cat("Discrepancy of",round(discrepancy,2),
        "between true eta,",round(true_eta_Intercept,2),
        "and what brms estimated,",round(eta_Intercept,2))
  }
  
  library(broom.mixed) #tidying methods for mixed models 
  broom.mixed::tidy(fit) #similar to just printing estimates
  
}
```


If estimating intercept, calculate the reference group location parameter.

```{r}
#| echo: false 
if (any(str_detect(row.names(estimates),"eta_Intercept"))) {
  
  #Grab the estimates column of the parameter estimates
  estimates<- as.data.frame( fixef(fit) )
  
  #Calculate trueVal of overall average location_parameter, returned as eta_Intercept by brms
  #brms returns estimates relative to baseline eta_Intercept, 
  #This is the estimated value of the location parameter (η) for the reference group. The #reference group is defined by the baseline levels of your categorical predictors (typically #the first level alphabetically or the reference level in your factors). For example, if your #factors are lab, age_group, gender, and targetLoad, the reference group might be:
  # lab == "Holcombe" (if "Holcombe" is first alphabetically)
  #age_group == "younger" ( "older" is first alphabetically), gender == "F"
  #targetLoad == 2 (if 2 is first)
  #Calculate that reference value
  
  #brms expresses estimates relative to a reference condition
  #If a factor is not modelled, then essentially it's using the average of all the levels of that condition, I think, so I need to include them all in the reference group, e.g. lab=c("Holcombe","Roudaia")
  reference_group <- data.frame(
    lab=c("Holcombe","Roudaia"),
    ageGroup = c("older","younger"),
    gender = c("F","M"),
    numTargets = c("two","three")
  )
  
  #brms' estimate for the reference group
  eta_Intercept<- estimates["eta_Intercept","Estimate"]
  
  eta_reference_group <- data_simulated |>  #filter by values in reference condition
    semi_join(reference_group, by = names(reference_group))
  #Calculate the average ground truth location_param for the reference condition
  true_eta_reference<- eta_reference_group |> 
    summarise(eta_avg = mean(location_param), .groups="drop")
  true_eta_reference<- mean(true_eta_reference$eta_avg)
  
  discrepancy<- eta_Intercept - true_eta_reference 
  if (abs(discrepancy)<0.1) {
    print("Success. Looks like I/brms calculated/estimated the reference group location parameter correctly.")
  } else {
    cat("Discrepancy between my calculation of eta_reference,",round(true_eta_reference,2),
        "and what brms calculated,",round(eta_Intercept,2),"of",round(discrepancy,2))
  }
}
```

Check brms' estimate of sigma.

```{r}
#| echo: false 

#Grab the estimates column of the parameter estimates
estimates<- as.data.frame( fixef(fit) )

#brms' estimate for the reference group
eta_logSigma<- estimates["logSigma_Intercept","Estimate"]

true_logSigma<- log(sigma)

discrepancy<- eta_logSigma - true_logSigma 
if (abs(discrepancy)<0.1) {
  print("Success. Looks like I/brms calculated/estimated the reference group logSigma correctly.")
} else {
  cat("Discrepancy of",round(discrepancy,2),
      "between true logSigma,",round(true_logSigma,2),
        "and what brms estimated,",round(eta_logSigma,2))
}
  
```

## Plot the predictions of the brms fit

For an example condition, model only one factor and only use data relevant to that

```{r}
#| echo: false 
#Reduce data_simulated to example_condition
example_condition<- data.frame( #reference_group 
  lab="Holcombe",
  ageGroup="older",
  gender="F",
  numTargets="three",
  obj_per_ring=8,
  subj_standardzd=0
)

data_to_model <- data_simulated |>
  semi_join(example_condition, by = names(example_condition))
```

Set up all the conditions to calculate the prediction for.

```{r}
#| echo: false 

# First set up all conditions
prediction_grid <- expand.grid(
  lab = unique(data_to_model$lab),
  ageGroup = unique(data_to_model$ageGroup),
  gender = unique(data_to_model$gender),
  numTargets = unique(data_to_model$numTargets),
  speed = seq(min(data_to_model$speed), max(data_to_model$speed), length.out = 40),
  obj_per_ring = unique(data_to_model$obj_per_ring),
  subj_standardzd = 0         # Use mean subject
)
prediction_conditions<- prediction_grid |> mutate( chance_rate =
                                                    1 / obj_per_ring )

# Filter prediction grid for data subset being modeled (example_condition)
prediction_subset <- prediction_conditions %>%
  semi_join(example_condition, by = names(example_condition))

#Not modeling lapse, so need to add it to prediction so prediction can be made
prediction_subset$lapseRate <- lapse

#Not modeling eta, so need to add it to prediction so prediction can be made
prediction_subset$eta <- true_eta

# Get fitted values (posterior mean and CI) for each condition
fitted_preds <- fitted(fit, newdata = prediction_subset)#, summary = TRUE)
#Merge y's with conditions
prediction_subset$p_correct <- fitted_preds[, "Estimate"]
prediction_subset$p_lower <- fitted_preds[, "Q2.5"]
prediction_subset$p_upper <- fitted_preds[, "Q97.5"]

```

Plot

```{r}
#| echo: false 

# Add a curve_type column to each data frame, for the legend
prediction_subset$curve_type <- "brms fit"
data_to_model$curve_type <- "generating model"
# Combine for plotting
plot_data <- bind_rows(
  prediction_subset %>% select(speed, p_correct, curve_type),
  data_to_model %>% select(speed, p_correct, curve_type)
)

hh<- ggplot(plot_data, 
            aes(x= speed, y= p_correct, color=curve_type, shape=ageGroup)) +
  geom_line() + #fit
  #geom_ribbon(aes(ymin = p_lower, ymax = p_upper), fill = "blue", alpha = 0.2, color = NA) +
  geom_line(data = data_to_model, #actual data generating model
            linetype="dotted") + geom_point(data=data_to_model) +
  # labs(
  #   x = "Speed (revolutions per second)",
  #   y = "P(Correct)",
  #   title = "curve is Prediction, dots are generative model",
  #   linetype= "Curve",
  #   subtitle = ""
  # ) +
  scale_color_manual(
    name = "",
    values = c("brms fit" = "red", "generating model" = "black")
  ) +
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

show(hh)
```


Look at what is happening during the chain, as explained on the [Stan forum](https://discourse.mc-stan.org/t/init-not-using-my-initial-values-and-seems-to-be-defaulting-to-0/39548)

```{r}
#| echo: false 
library(ggmcmc)

chainchainchain<- ggmcmc::ggs(fit, burnin = TRUE)

#Label the different kinds of parameters differently so can use 
#facet_grid to put them on separate panels
chainchainchain <- chainchainchain |>  filter(Parameter != "lprior") |>
  mutate(
    paramGroup = case_when(
      Parameter %in% c("b_logSigma_Intercept", "b_eta_Intercept") ~ 1,
      TRUE ~ 2
    )
  )

chainchainchain |>
  filter(Iteration < 100) |> #Because nothing happens after first couple hundred
  ggplot(aes(x = Iteration, y = value, linetype = factor(Chain),
             color = Parameter)) +
  facet_grid(paramGroup ~ ., scales="free") +
  geom_line()+
  ggtitle("")+
  theme_bw()

```
Plot sigma specifically.
```{r}
#| echo: false 
chainchainchain |>
  filter(str_detect(Parameter, "b_logSigma_Intercept")) |>
  filter(Iteration < 300) |> #Because nothing happens after first couple hundred
  ggplot(aes(x = Iteration, y = value, linetype = factor(Chain),
             color = Parameter)) +
  facet_grid(paramGroup ~ ., scales="free") +
  geom_line() +
  geom_hline(yintercept=log(sigma), alpha=.5,linetype="dotted")+
  ggtitle("")+
  theme_bw()

```

```{r}
#| echo: false 

chainchainchain |>
  filter( str_detect(Parameter, "igma") ) |>
  #filter(value < .4) |>
  ggplot(aes(x = value, color=Parameter, linetype = factor(Chain))) +
  facet_grid(Parameter~., scales="free") +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept=log(sigma), alpha=.5,linetype="dotted")+
  labs(title = "Posterior Density")
```
Use brms' built-in plot to show posterior and chains.
```{r}
#| echo: false 
plot(fit)
```

Brms'  conditional_effects method visualizes the model-implied regression curve:
```{r}
#| echo: false 
#interactive plots so not sure will render to html
#plot(conditional_effects(fit), points = TRUE)
```
