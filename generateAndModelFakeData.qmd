---
title: "exampleBrmsPsychometric"
format: html
editor: visual
---

## Load R libraries

hello everybody

This code initially based on <https://discourse.mc-stan.org/t/fitting-lapsing-psychometric-functions-with-brms/5762/2> .

```{r}
library(tidyverse)
library(brms)
```

## Random Notes (Vince; Extensive use of Microsoft Copilot)

### Logistic Function

`plogis(x)` is the default logistic function with **location** parameter $\mu = 0$ and **scale** parameter $s = 1$

$$
plogis(x) = \frac{1}{1+e^{-(x-\mu)/s}} = \frac{1}{1+e^{-x}}
$$

The **location** parameter refers to where on the x-axis the y axis is 0.5 (in our context, 50% correct) and the scale parameter is how steep the curve is about this 50% mark (close to 0 is VERY steep, higher values is not so steep). The default curve is below, but obviously our x-axis is speed so it's not going to be negative! also, our x axis is 'bounded' by a much smaller interval. (we know that the min speed by rps in our experiment is 0.02 and the max is 1.8. hence we have to adjust the location and scale.

```{r}
plot(seq(-5, 5, by = 0.1), plogis(seq(-5, 5, by = 0.1)), type = "l", col = "blue", lwd = 2, main = "Default Logistic Curve using plogis()", xlab = "x", ylab = "Probability")

```

### Flipping the curve

but first we need to flip the curve because the logistic function is an increasing function from 0 to 1, but our experiment we want as x increases (speed) then the y axis (% correct) decreases

-   we take `1-plogis(x)` (this doesn't affect estimation)

### Location

From the methods, it appears that the revolutions per second (rps) of any given trial is bounded by $speed \in [0.02,1.8]$

-   This location parameter would be based off the literature and a given condition on where this 50% mark would be, but i left it at 0.91 (halfway between 0.02 and 1.8) for now as something generic that can be changed.

### Scale

The Scale parameter is a bit more complicated. Using microsoft copilot, i generated how changing the scale affects the function. by default it is scale = 1 but, again, this is not appropriate for how small our interval is.

```{r}

## CODE FROM MICROSOFT COPILOT!

# Define the logistic function using plogis
logistic_function <- function(speed, chanceRate, lapse, location, scale) {
  chanceRate + (1 - chanceRate - lapse) * (1 - plogis((speed - location) / scale))
}

# Parameters
chanceRate <- 0.02
lapse <- 0.03
location <- 0.91 #placeholder, it would be actually defined by the literature (50% point)

# Speed range
speed <- seq(0, 2, length.out = 400)

# Different scale values to plot
scale_values <- c(0.01, 0.05, 0.1, 0.2)

# Plotting the logistic functions with different scale values
plot(NULL, xlim = c(0, 2), ylim = c(0, 1), xlab = "Speed", ylab = "Probability", main = "Logistic Functions with Different Scale Values")
colors <- rainbow(length(scale_values))

for (i in seq_along(scale_values)) {
  scale <- scale_values[i]
  lines(speed, logistic_function(speed, chanceRate, lapse, location, scale), col = colors[i], lwd = 2)
}

legend("bottomright", legend = paste("Scale =", scale_values), col = colors, lwd = 2)
```

Given the purpose of this is to test how our analysis performs, i am unsure what scale to use. the original data generation was between -5 and 5 with a scale of 1. The the equivalent scale for the interval of \[0.02, 1.8\] is `scale = 0.178`but let's go with `scale = 0.2`

### Bounding the y-axis of the curve

With the probability of just guessing being `chanceRate = 0.02` for example, we would like to move our curve up to reflect this. so we add `chanceRate` to the equation.

-   Now we have `chanceRate + 1-plogis((speed-location)/scale).`

-   If we left it like this, the ceiling would ofcourse go above 1 which makes no sense, we must 'squish' the curve. We also need to 'squish' it with the `lapse` rate as even the best performers are expected to not perform perfectly and have small perceptual lapses.

-   `chanceRate + (1-chanceRate-lapse)*(1-plogis((speed-location)/scale).`

### Alternate parametrization

In the `brms` analysis, the parameters estimated is $\beta_0$ and $\beta_1$ in which compose the formula of eta

$$
plogis(x) = \frac{1}{1+e^{-\eta}}= \frac{1}{1+e^{-(\beta_0 +\beta_1x_{speed})}}
$$

Which are equivalent to the coefficients that we predict

$$
\eta=\beta_0 +\beta_1x_{speed}
$$

Using the **scale** and **location** parameters this gives us $\beta_0 = -\mu/s$ and $\beta_1= 1/s$

All of this is to say that with $\mu=0.91$ and $s=0.2$ , we have the parameters we are trying to get to be $\beta_0 =-4.55$ and $\beta_1=5$ . remember these numbers.

## Generate a fake dataset

```{r}
#| echo: false
set.seed(999)
chanceRate <- .02 #probablity of correct just guessing
lapse <- .03 #even most attentive person may zone out in trial and guess
numSs <- 3 #small number as the brm takes ages

location <- 0.91 #where on the x-axis (speed) does the y axis reach 50%
scale <- 0.2 #


conditionsAndIVs <- 
  tidyr::expand_grid(
    speed = seq(.02,1.8, length.out = 10), #changed min and max speed to match our data
    rep = seq(1, 5),
    subj = seq(1, numSs)
  )

fakedata<- conditionsAndIVs

#create a new column
fakedata<- fakedata %>% mutate(
  probabilityCorrect = chanceRate + (1-chanceRate-lapse)*(1-plogis((speed-location)/scale))
)

fakedata<- fakedata %>% mutate(
          correct = rbinom(n=length(probabilityCorrect), size = 1, prob = probabilityCorrect)
         )

fakedata$chanceRate <- chanceRate #Putting chanceRate in the brms formula, brms will know to look for chanceRate in the data dataframe

```

## Plot data

```{r}
#| echo: false
ggplot(fakedata, aes(x=speed,y=correct)) + 
  stat_summary( fun="mean", geom="point" ) +
  facet_wrap(.~subj)
```

## Set up the model

```{r}
#| echo: false
#| 
# Set up formula model for fitting
myformula <- brms::brmsformula(
  correct ~ chanceRate + (1-chanceRate-lapse) * (1-inv_logit(eta)),
  eta ~ 1 + speed,
  lapse ~ 1, 
  family = bernoulli(link="identity"),
  nl = TRUE
) 

# Set up priors
lapse_rate_bounds <- c(0,.4)

mypriors <- c(
  brms::prior(student_t(7, 0, 10), class = "b", nlpar = "eta"),
  brms::prior(beta(1, 1), nlpar = "lapse", lb = 0, ub = .1)
)

# Plot priors
# not done yet

```

\*to-do: set chanceRate to 1/numobjects, and also check lapse parametrization

-   (vince) need to adjust priors to new model?

## Do the fit

```{r}
#! echo:false

fit2 <- brm(
  myformula,
  data = fakedata,
  init = 0,
  control = list(adapt_delta = 0.99),
  prior = mypriors,
)

print(fit2)
```

Recall that $\beta_0 =-4.55$ and $\beta_1=5$. While these are within the 95% confidence intervals, the point estimates are quite off

## Show fit

```{r}
predict_interval_brms <- predict(fit2, re_formula = NULL) #or use fitted to not take into account uncertainty of observations
#head(predict_interval_brms)
dataWithModelPredictions<- cbind(fakedata,predict_interval_brms)

ggplot(dataWithModelPredictions, aes(x=speed,y=correct)) + 
  stat_summary( fun="mean", geom="point" ) +
  geom_line( aes(x=speed, y= Estimate) ) +
  facet_wrap(.~subj)

```

In the above fit, I think it is overplotting the lines many times because there are multiple trials at each speed. So, should reduce the fakedata to unique values and then plot - and eventually interpolate to show smooth curves, and maybe annotate with lapse rate.

```{r}
conditionsEachTrial<- fakedata %>% select(speed,subj,chanceRate)
conditionsUniq <- unique(conditionsEachTrial)
prediction <- predict(fit2, newdata=conditionsUniq, re_formula = NULL) 
prediction<- fitted(fit2, newdata=conditionsUniq, re_formula = NULL) 

predictions<- cbind(conditionsUniq, prediction)
  
  
ggplot(dataWithModelPredictions, aes(x=speed,y=correct)) + 
  stat_summary( fun="mean", geom="point" ) +
  geom_line(data=predictions, aes(x=speed, y= Estimate) ) +
  geom_ribbon(data=predictions, aes(y=Estimate, ymin = Q2.5, ymax = Q97.5),
             alpha = .3, fill = "red") +
  facet_wrap(.~subj)
```

Why are Q2.7 and Q97.5 of predict always 0 and 1 but fitted is a continuous value? Maybe it is a consequence of predict not taking into account the uncertainty of the data.

## Resources

McElreath's Statistical Rethinking rewritten in brms and ggplot2 with multilevel models <https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multilevel-models.html#multilevel-posterior-predictions>

Cheat sheet especially for more advanced things: <https://michael-franke.github.io/Bayesian-Regression/practice-sheets/11a-cheat-sheet.html>

<https://cu-psych-computing.github.io/cu-psych-comp-tutorial/tutorials/r-extra/accelerated-ggplot2/ggplot_summer2018_part2/>

<https://kzee.github.io/PlotFixef_Demo.html#how-do-the-plots-using-lme4-and-brms-compare>

Next, we will use the `fitted()` function in `brms` to generate predictions and the 95% credibility interval. We will append these predicted values to our `mydatab` dataframe.

Note that `brms` features both a `fitted()` function and a `predict()` function, but they will return different information. The fitted line should be the same for both, but the credibility intervals differ. `fitted()` takes uncertainty of the estimation of the fitted line into account, whereas `predict()` takes into account both uncertainty about the estimation of the fitted line and uncertainty about the data. Thus, `predict()` in `brms` will yield a wider interval. `fitted()` closely matches the predicted interval we get from the `lmer()` model.
